{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Well rSVD Preprocessing\n",
    "\n",
    "[link to official documentation](https://polymathic-ai.org/the_well/tutorials/dataset/)  \n",
    "[link to original paper](https://proceedings.neurips.cc/paper_files/paper/2024/file/4f9a5acd91ac76569f2fe291b1f4772b-Paper-Datasets_and_Benchmarks_Track.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import pprint as pp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from einops import rearrange\n",
    "from tqdm import tqdm\n",
    "\n",
    "from the_well.benchmark.metrics import VRMSE\n",
    "from the_well.data import WellDataset\n",
    "from the_well.utils.download import well_download\n",
    "from the_well.data import WellDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Add the project root directory to Python path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root.absolute()))\n",
    "\n",
    "from src.helpers import *\n",
    "\n",
    "debug = False\n",
    "n_iters = 5\n",
    "dataset = 'planetswe'\n",
    "n_steps = {'planetswe': 1008, 'active_matter': 81}\n",
    "n_rank = {'active_matter': 50, 'planetswe': 75}\n",
    "\n",
    "save_dir = project_root / 'datasets' / 'the_well_custom' / dataset\n",
    "save_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1008, 256, 512, 3])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load data from online, when using in practice we'll have to download the dataset\n",
    "train_data = WellDataset(\n",
    "    well_base_path=Path('/data') / 'alexey' / 'the_well',\n",
    "    well_dataset_name=dataset,\n",
    "    well_split_name=\"train\",\n",
    "    n_steps_input=n_steps[dataset],\n",
    "    n_steps_output=0,\n",
    "    use_normalization=False,\n",
    ")\n",
    "\n",
    "valid_data = WellDataset(\n",
    "    well_base_path=Path('/data') / 'alexey' / 'the_well',\n",
    "    well_dataset_name=dataset,\n",
    "    well_split_name=\"valid\",\n",
    "    n_steps_input=n_steps[dataset],\n",
    "    n_steps_output=0,\n",
    "    use_normalization=False,\n",
    ")\n",
    "\n",
    "test_data = WellDataset(\n",
    "    well_base_path=Path('/data') / 'alexey' / 'the_well',\n",
    "    well_dataset_name=dataset,\n",
    "    well_split_name=\"test\",\n",
    "    n_steps_input=n_steps[dataset],\n",
    "    n_steps_output=0,\n",
    "    use_normalization=False,\n",
    ")\n",
    "\n",
    "im_shape = train_data[0]['input_fields'].shape\n",
    "im_rows, im_cols, im_dim = im_shape[1], im_shape[2], im_shape[3]\n",
    "print(im_shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96\n",
      "torch.Size([1008, 393216])\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "combine_all = False\n",
    "train_mats_raw = create_mats(train_data, combine_all=combine_all, debug=debug)\n",
    "valid_mats_raw = create_mats(valid_data, combine_all=combine_all, debug=debug)\n",
    "test_mats_raw = create_mats(test_data, combine_all=combine_all, debug=debug)\n",
    "\n",
    "print(len(train_mats_raw))\n",
    "print(train_mats_raw[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_mat = torch.cat(train_mats_raw + valid_mats_raw + test_mats_raw)\n",
    "\n",
    "print(full_mat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "U_full, S_full, V_full = generate_SVD(full_mat, n_rank=n_rank[dataset], n_iters=n_iters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3024, 75])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "full_pod = create_pod(full_mat, V_full)\n",
    "\n",
    "print(full_pod.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "full_pod_scaled, full_scaler = scale_pod(full_pod)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverse POD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "full_mat_hat = inverse_pod(full_pod_scaled, full_scaler, V_full)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training POD errors:\n",
      "Error for i=0 is 3.37%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print_errors([full_mat], [full_mat_hat], mean_relative_error, \"Training POD errors:\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate into Training, Validation, and Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_num 806\n",
      "valid_num 100\n",
      "test_num 100\n"
     ]
    }
   ],
   "source": [
    "\n",
    "total_tracks = int(full_mat.shape[0] / n_steps[dataset])\n",
    "\n",
    "train_num = int(0.8*n_steps[dataset])\n",
    "valid_num = int(0.1*n_steps[dataset])\n",
    "test_num = int(0.1*n_steps[dataset])\n",
    "\n",
    "print(\"train_num\", train_num)\n",
    "print(\"valid_num\", valid_num)\n",
    "print(\"test_num\", test_num)\n",
    "\n",
    "train_save = []\n",
    "valid_save = []\n",
    "test_save = []\n",
    "\n",
    "train_pods_save = []\n",
    "valid_pods_save = []\n",
    "test_pods_save = []\n",
    "\n",
    "for track_num in range(total_tracks):\n",
    "    track_pod_scaled = full_pod_scaled[track_num*n_steps[dataset]:(track_num+1)*n_steps[dataset],:]\n",
    "    track = full_mat[track_num*n_steps[dataset]:(track_num+1)*n_steps[dataset],:]\n",
    "\n",
    "    train_pod_scaled = track_pod_scaled[0:train_num]\n",
    "    valid_pod_scaled = track_pod_scaled[train_num:train_num+valid_num]\n",
    "    test_pod_scaled = track_pod_scaled[train_num+valid_num:]\n",
    "\n",
    "    train = track[0:train_num]\n",
    "    val = track[train_num:train_num+valid_num]\n",
    "    test = track[train_num+valid_num:]\n",
    "\n",
    "    train_pods_save.append(train_pod_scaled)\n",
    "    valid_pods_save.append(valid_pod_scaled)\n",
    "    test_pods_save.append(test_pod_scaled)\n",
    "\n",
    "    train_save.append(train)\n",
    "    valid_save.append(val)\n",
    "    test_save.append(test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Per-Track and Per-Split Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train POD errors:\n",
      "Error for i=0 is 3.30%\n",
      "Error for i=1 is 3.49%\n",
      "Error for i=2 is 3.17%\n",
      "\n",
      "Validation POD errors:\n",
      "Error for i=0 is 3.16%\n",
      "Error for i=1 is 4.23%\n",
      "Error for i=2 is 2.97%\n",
      "\n",
      "Testing POD errors:\n",
      "Error for i=0 is 3.37%\n",
      "Error for i=1 is 4.83%\n",
      "Error for i=2 is 3.01%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_save_hat = inverse_pods(train_pods_save, full_scaler, V_full)\n",
    "print_errors(train_save, train_save_hat, mean_relative_error, \"Train POD errors:\")\n",
    "\n",
    "valid_save_hat = inverse_pods(valid_pods_save, full_scaler, V_full)\n",
    "print_errors(valid_save, valid_save_hat, mean_relative_error, \"Validation POD errors:\")\n",
    "\n",
    "test_save_hat = inverse_pods(test_pods_save, full_scaler, V_full)\n",
    "print_errors(test_save, test_save_hat, mean_relative_error, \"Testing POD errors:\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create directories\n",
    "(save_dir / 'metadata').mkdir(exist_ok=True)\n",
    "(save_dir / 'full').mkdir(exist_ok=True)\n",
    "(save_dir / 'pod').mkdir(exist_ok=True)\n",
    "\n",
    "# Save scaler, V_full, and image metadata\n",
    "torch.save(V_full, save_dir / 'metadata' / 'V.pt')\n",
    "torch.save(full_scaler, save_dir / 'metadata' / 'scaler.pt')\n",
    "torch.save((im_rows, im_cols, im_dim), save_dir / 'metadata' / 'im_dims.pt')\n",
    "\n",
    "# Save full and pod tracks\n",
    "for i in range(total_tracks):\n",
    "    torch.save(train_save[i], save_dir / 'full' / f'train_{i}.pt')\n",
    "    torch.save(train_pods_save[i], save_dir / 'pod' / f'train_{i}.pt')\n",
    "    torch.save(valid_save[i], save_dir / 'full' / f'valid_{i}.pt')\n",
    "    torch.save(valid_pods_save[i], save_dir / 'pod' / f'valid_{i}.pt')\n",
    "    torch.save(test_save[i], save_dir / 'full' / f'test_{i}.pt')\n",
    "    torch.save(test_pods_save[i], save_dir / 'pod' / f'test_{i}.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([806, 393216])\n",
      "torch.Size([806, 256, 512, 3])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_loaded = torch.load(save_dir / 'full' / 'train_0.pt', weights_only=False)\n",
    "im_rows, im_cols, im_dim = torch.load(save_dir / 'metadata' / 'im_dims.pt', weights_only=False)\n",
    "print(train_loaded.shape)\n",
    "train_loaded_shaped = rearrange(train_loaded, \"t (r c d) -> t r c d\", t=int(n_steps[dataset]*0.8), r=im_rows, c=im_cols, d=im_dim)\n",
    "print(train_loaded_shaped.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
