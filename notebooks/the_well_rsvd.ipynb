{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Well rSVD Preprocessing\n",
    "\n",
    "[link to official documentation](https://polymathic-ai.org/the_well/tutorials/dataset/)  \n",
    "[link to original paper](https://proceedings.neurips.cc/paper_files/paper/2024/file/4f9a5acd91ac76569f2fe291b1f4772b-Paper-Datasets_and_Benchmarks_Track.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pprint as pp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from einops import rearrange\n",
    "from tqdm import tqdm\n",
    "\n",
    "from the_well.benchmark.metrics import VRMSE\n",
    "from the_well.data import WellDataset\n",
    "from the_well.utils.download import well_download\n",
    "from the_well.data import WellDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "debug = True\n",
    "n_rank = 50\n",
    "n_iters = 5\n",
    "dataset = 'planetswe'\n",
    "n_steps = {'planetswe': 1008, 'active_matter': 81}\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mae = lambda datatrue, datapred: (datatrue - datapred).abs().mean()\n",
    "mse = lambda datatrue, datapred: (datatrue - datapred).pow(2).sum(axis = -1).mean()\n",
    "mre = lambda datatrue, datapred: ((datatrue - datapred).pow(2).sum(axis = -1).sqrt() / (datatrue).pow(2).sum(axis = -1).sqrt()).mean()\n",
    "num2p = lambda prob : (\"%.2f\" % (100*prob)) + \"%\"\n",
    "\n",
    "def min_max_scale(tensor, feature_range=(0, 1)):\n",
    "    \"\"\"\n",
    "    Scale a tensor to a given feature range using min-max normalization.\n",
    "    \n",
    "    Args:\n",
    "        tensor (torch.Tensor): Input tensor to be scaled\n",
    "        feature_range (tuple): Desired range of transformed data (default: (0, 1))\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Scaled tensor\n",
    "        tuple: (min, max) values used for scaling (for inverse transformation)\n",
    "    \"\"\"\n",
    "    # Ensure the input is a tensor\n",
    "    if not isinstance(tensor, torch.Tensor):\n",
    "        tensor = torch.tensor(tensor, dtype=torch.float32)\n",
    "    \n",
    "    # Calculate min and max\n",
    "    t_min = tensor.min()\n",
    "    t_max = tensor.max()\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    t_range = t_max - t_min\n",
    "    if t_range == 0:  # all values are the same\n",
    "        t_range = 1\n",
    "    \n",
    "    # Scale to [0, 1] first\n",
    "    scaled = (tensor - t_min) / t_range\n",
    "    \n",
    "    # Then scale to feature_range\n",
    "    min_range, max_range = feature_range\n",
    "    scaled = scaled * (max_range - min_range) + min_range\n",
    "    \n",
    "    return scaled, (t_min, t_max)\n",
    "\n",
    "def inverse_min_max_scale(scaled_tensor, original_min_max, feature_range=(0, 1)):\n",
    "    \"\"\"\n",
    "    Inverse transformation of min-max scaling.\n",
    "    \n",
    "    Args:\n",
    "        scaled_tensor (torch.Tensor): Scaled tensor to transform back\n",
    "        original_min_max (tuple): (min, max) values from original scaling\n",
    "        feature_range (tuple): Range used in original scaling (default: (0, 1))\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Tensor in original scale\n",
    "    \"\"\"\n",
    "    t_min, t_max = original_min_max\n",
    "    min_range, max_range = feature_range\n",
    "    \n",
    "    # First scale back to [0, 1] range\n",
    "    normalized = (scaled_tensor - min_range) / (max_range - min_range)\n",
    "    \n",
    "    # Then scale back to original range\n",
    "    original = normalized * (t_max - t_min) + t_min\n",
    "    \n",
    "    return original\n",
    "\n",
    "def create_mats(the_well_data, combine_all=False, debug=False):\n",
    "    im_shape = the_well_data[0][\"input_fields\"].shape\n",
    "    n_steps, im_rows, im_cols, im_dim = im_shape[0], im_shape[1], im_shape[2], im_shape[3]\n",
    "\n",
    "    mats = []\n",
    "    for i in range(len(the_well_data)):\n",
    "        data = rearrange(the_well_data[i][\"input_fields\"], \"t r c d -> t (r c d)\", t=n_steps, r=im_rows, c=im_cols, d=im_dim)\n",
    "        mats.append(data)\n",
    "        if debug:\n",
    "            break\n",
    "    if combine_all:\n",
    "        mats = [torch.cat(mats, dim=0)]\n",
    "    return mats\n",
    "\n",
    "def generate_SVDs(mats, n_rank=50, n_iters=2):\n",
    "    Us, Ss, Vs = ([], [], [])\n",
    "    for mat in mats:\n",
    "        U, S, V = torch.svd_lowrank(mat, n_rank, n_iters)\n",
    "        Us.append(U)\n",
    "        Ss.append(S)\n",
    "        Vs.append(V)\n",
    "    return Us, Ss, Vs\n",
    "\n",
    "def create_pods(mats, V_mats):\n",
    "    pods = []\n",
    "    for mat, V in zip(mats, V_mats):\n",
    "        pod = mat @ V\n",
    "        pods.append(pod)\n",
    "    return pods\n",
    "\n",
    "def scale_pods(pods):\n",
    "    pods_scaled = []\n",
    "    scalers_l = []\n",
    "    for pod in pods:\n",
    "        pod_scaled, scalers = min_max_scale(pod)\n",
    "        pods_scaled.append(pod_scaled)\n",
    "        scalers_l.append(scalers)\n",
    "    return pods_scaled, scalers_l\n",
    "\n",
    "def inverse_pod(pods_scaled, scalers_l, V_mats):\n",
    "    mats_hat = []\n",
    "    for pod_scaled, scalers, V_mat in zip(pods_scaled, scalers_l, V_mats):\n",
    "        mat_hat = inverse_min_max_scale(pod_scaled, scalers)\n",
    "        mat_hat = mat_hat @ V_mat.T\n",
    "        mats_hat.append(mat_hat)\n",
    "    return mats_hat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load data from online, when using in practice we'll have to download the dataset\n",
    "train_data = WellDataset(\n",
    "    well_base_path=Path('/data') / 'alexey' / 'the_well',\n",
    "    well_dataset_name=dataset,\n",
    "    well_split_name=\"train\",\n",
    "    n_steps_input=n_steps[dataset],\n",
    "    n_steps_output=0,\n",
    "    use_normalization=False,\n",
    ")\n",
    "\n",
    "valid_data = WellDataset(\n",
    "    well_base_path=Path('/data') / 'alexey' / 'the_well',\n",
    "    well_dataset_name=dataset,\n",
    "    well_split_name=\"valid\",\n",
    "    n_steps_input=n_steps[dataset],\n",
    "    n_steps_output=0,\n",
    "    use_normalization=False,\n",
    ")\n",
    "\n",
    "test_data = WellDataset(\n",
    "    well_base_path=Path('/data') / 'alexey' / 'the_well',\n",
    "    well_dataset_name=dataset,\n",
    "    well_split_name=\"test\",\n",
    "    n_steps_input=n_steps[dataset],\n",
    "    n_steps_output=0,\n",
    "    use_normalization=False,\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1008, 393216])\n",
      "torch.Size([1008, 393216])\n",
      "torch.Size([1008, 393216])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "combine_all = True\n",
    "train_mats = create_mats(train_data, combine_all=combine_all, debug=debug)\n",
    "valid_mats = create_mats(valid_data, combine_all=combine_all, debug=debug)\n",
    "test_mats = create_mats(test_data, combine_all=combine_all, debug=debug)\n",
    "\n",
    "print(train_mats[0].shape)\n",
    "print(valid_mats[0].shape)\n",
    "print(test_mats[0].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "U_trains, S_trains, V_trains = generate_SVDs(train_mats, n_rank=n_rank, n_iters=n_iters)\n",
    "U_valids, S_valids, V_valids = generate_SVDs(valid_mats, n_rank=n_rank, n_iters=n_iters)\n",
    "U_tests, S_tests, V_tests = generate_SVDs(test_mats, n_rank=n_rank, n_iters=n_iters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1008, 50])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_pods = create_pods(train_mats, V_trains)\n",
    "valid_pods = create_pods(valid_mats, V_valids)\n",
    "test_pods = create_pods(test_mats, V_tests)\n",
    "\n",
    "print(train_pods[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_pods_scaled, train_scalers = scale_pods(train_pods)\n",
    "valid_pods_scaled, valid_scalers = scale_pods(valid_pods)\n",
    "test_pods_scaled, test_scalers = scale_pods(test_pods)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverse POD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_mats_hat = inverse_pod(train_pods_scaled, train_scalers, V_trains)\n",
    "valid_mats_hat = inverse_pod(valid_pods_scaled, valid_scalers, V_valids)\n",
    "test_mats_hat = inverse_pod(test_pods_scaled, test_scalers, V_tests)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training POD errors:\n",
      "Error for i=0 is 1.34%\n",
      "\n",
      "Validation POD errors:\n",
      "Error for i=0 is 1.45%\n",
      "\n",
      "Testing POD errors:\n",
      "Error for i=0 is 1.18%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def print_errors(true_l, pred_l, error_f, title):\n",
    "    print(title)\n",
    "    for i, (true, pred) in enumerate(zip(true_l, pred_l)):\n",
    "        print(f\"Error for i={i} is {num2p(error_f(true, pred))}\")\n",
    "    print()\n",
    "\n",
    "print_errors(train_mats, train_mats_hat, mre, \"Training POD errors:\")\n",
    "print_errors(valid_mats, valid_mats_hat, mre, \"Validation POD errors:\")\n",
    "print_errors(test_mats, test_mats_hat, mre, \"Testing POD errors:\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
